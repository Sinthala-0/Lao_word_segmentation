# Segmentation or Tokenization
## This is an approch to segment unsegmented languages like Laos, Chinese, Thai
* In this program I have used maximal matching method(longest matching algorithm) to segment Lao words

### References:

Modified code from: ->[reference](https://medium.com/@anshul16/maximum-matching-word-segmentation-algorithm-python-code-3444fe4bd6f9) <br>
Source about other methods on tokeniztion: ->[here](https://www.academia.edu/265589/A_Comparative_Study_on_Thai_Word_Segmentation_Approaches) <br>
More info about Natural Language processing(NLP): Text Segmentation Using Dictionary Based Algorithms. ->Click here: [reference](https://medium.com/@phylypo/nlp-text-segmentation-using-dictionary-based-algorithms-6d0a45a76c08)
